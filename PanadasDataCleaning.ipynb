{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PanadasDataCleaning",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNk4YFQ2TLfNLuw020nh4g9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkaustubh/pandas-data-cleaning/blob/master/PanadasDataCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP_mvoi5X0-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3352beb9-171e-488f-ce26-47990e1694ed"
      },
      "source": [
        "!git clone https://github.com/mkaustubh/pandas-data-cleaning.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pandas-data-cleaning'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 39 (delta 8), reused 18 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (39/39), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AnpQsedtLdG"
      },
      "source": [
        "**This notebook is an exercise in the [Data Cleaning](https://www.kaggle.com/learn/data-cleaning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/handling-missing-values).**\n",
        "\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RY3ysOHVlVW"
      },
      "source": [
        "# 1) Take a first look at the data\n",
        "\n",
        "Run the next code cell to load in the libraries and dataset you'll use to complete the exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg0jUdzbV2zk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "sf_permits = pd.read_csv('pandas-data-cleaning/archive/Building_Permits.csv')\n",
        "sf_permits.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0bLtazhcnIr"
      },
      "source": [
        "# 2) How many missing data points do we have?\n",
        "\n",
        "What percentage of the values in the dataset are missing?  Your answer should be a number between 0 and 100.  (If 1/4 of the values in the dataset are missing, the answer is 25.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYtLbofzcoCh"
      },
      "source": [
        "percent_missing = sf_permits.isnull().sum().sum()/sf_permits.size*100\n",
        "percent_missing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K7Q75IdfI5f"
      },
      "source": [
        "# 4) Drop missing values: rows\n",
        "\n",
        "If you removed all of the rows of `sf_permits` with missing values, how many rows are left?\n",
        "\n",
        "**Note**: Do not change the value of `sf_permits` when checking this.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA1YwM3HfPu2"
      },
      "source": [
        "sf_permits.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNDv9a5Lfflg"
      },
      "source": [
        "# 5) Drop missing values: columns\n",
        "\n",
        "Now try removing all the columns with empty values.  \n",
        "- Create a new DataFrame called `sf_permits_with_na_dropped` that has all of the columns with empty values removed.  \n",
        "- How many columns were removed from the original `sf_permits` DataFrame? Use this number to set the value of the `dropped_columns` variable below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9wYhsLBfgwg"
      },
      "source": [
        "sf_permits_with_na_dropped = sf_permits.dropna(axis=1)\n",
        "dropped_columns = sf_permits.shape[1]-sf_permits_with_na_dropped.shape[1]\n",
        "dropped_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riWEQMjfgQf7"
      },
      "source": [
        "# 6) Fill in missing values automatically\n",
        "\n",
        "Try replacing all the NaN's in the `sf_permits` data with the one that comes directly after it and then replacing any remaining NaN's with 0.  Set the result to a new DataFrame `sf_permits_with_na_imputed`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BWjpqN0gTmX"
      },
      "source": [
        "sf_permits_with_na_imputed = sf_permits.fillna(method='bfill', axis=0).fillna(0)\n",
        "sf_permits_with_na_imputed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy0TIK7LGCxM"
      },
      "source": [
        "In this exercise, you'll apply what you learned in the **Scaling and normalization** tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO4g9aI1GyTn"
      },
      "source": [
        "# Get our environment set up\n",
        "\n",
        "To practice scaling and normalization, we're going to use a [dataset of Kickstarter campaigns](https://www.kaggle.com/kemical/kickstarter-projects). (Kickstarter is a website where people can ask people to invest in various projects and concept products.)\n",
        "\n",
        "The next code cell loads in the libraries and dataset we'll be using. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuYXmzMLGzBv"
      },
      "source": [
        "# modules we'll use\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for Box-Cox Transformation\n",
        "from scipy import stats\n",
        "\n",
        "# for min_max scaling\n",
        "from mlxtend.preprocessing import minmax_scaling\n",
        "\n",
        "# plotting modules\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# read in all our data\n",
        "kickstarters_2017 = pd.read_csv(\"pandas-data-cleaning/archive/ks-projects-201801.csv\")\n",
        "kickstarters_2017"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6BRVVXxLNxE"
      },
      "source": [
        "# 1) Practice scaling\n",
        "\n",
        "Begin by running the code cell below to create a DataFrame `original_goal_data` containing the \"goal\" column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY-la7jaLTqh"
      },
      "source": [
        "# select the usd_goal_real column\n",
        "original_goal_data = pd.DataFrame(kickstarters_2017.goal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTwRXK6cLdJo"
      },
      "source": [
        "Use `original_goal_data` to create a new DataFrame `scaled_goal_data` with values scaled between 0 and 1. You must use the `minimax_scaling()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY1cdNgxLdtx"
      },
      "source": [
        "scaled_goal_data = minmax_scaling(original_goal_data, columns=['goal'])\n",
        "scaled_goal_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDNRvKQSLo3n"
      },
      "source": [
        "# 2) Practice normalization\n",
        "\n",
        "Now you'll practice normalization. We begin by normalizing the amount of money pledged to each campaign."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f8SfmxOLwZn"
      },
      "source": [
        "# get the index of all positive pledges (Box-Cox only takes positive values)\n",
        "index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0\n",
        "\n",
        "# get only positive pledges (using their indexes)\n",
        "positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]\n",
        "\n",
        "# normalize the pledges (w/ Box-Cox)\n",
        "normalized_pledges = pd.Series(stats.boxcox(positive_pledges)[0], name='usd_pledged_real', index=positive_pledges.index)\n",
        "\n",
        "# plot both together to compare\n",
        "fig, ax=plt.subplots(1,2,figsize=(15,3))\n",
        "sns.distplot(positive_pledges, ax=ax[0])\n",
        "ax[0].set_title(\"Original Data\")\n",
        "sns.distplot(normalized_pledges, ax=ax[1])\n",
        "ax[1].set_title(\"Normalized data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqFk9kpFYrLI"
      },
      "source": [
        "# Get our environment set up\n",
        "\n",
        "The first thing we'll need to do is load in the libraries and dataset we'll be using. We'll be working with a dataset containing information on earthquakes that occured between 1965 and 2016."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4k1DkbJabuN"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "\n",
        "# read in our data\n",
        "earthquakes = pd.read_csv(\"pandas-data-cleaning/archive/database.csv\")\n",
        "earthquakes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmt5VISub0r5"
      },
      "source": [
        "# 1) Check the data type of our date column\n",
        "\n",
        "You'll be working with the \"Date\" column from the `earthquakes` dataframe.  Investigate this column now: does it look like it contains dates?  What is the dtype of the column?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HHxNf00b2mC"
      },
      "source": [
        "# TODO: Your code here!\n",
        "earthquakes.Date.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6YDGO12cPkB"
      },
      "source": [
        ""
      ]
    }
  ]
}